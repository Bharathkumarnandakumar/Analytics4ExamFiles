# Analytics4ExamFiles
Text Summarization
A process of creating a short, coherent, and fluent summary of a longer text document and involves the outlining of the text’s major points
The technique that shortens a long piece of content with main points outlined that gives an idea of the whole content 
Natural language processing aka NLP ( the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can) used in Text Summarization
Need:
Saves Time - By generating automatic summaries, text summarization helps content editors save time and effort
Instant Response - It reduces the user’s effort involved in exacting the relevant information
Increases Productivity Level –  It enables the user to scan through the contents of a text for accurate, brief, and precise information
Ensures All Important Facts are Covered - The human eye can miss crucial details; however, automatic software does not

Overview:
Generative Pre-trained Transformer 2 (GPT-2) is an open-source artificial intelligence created by OpenAI in February 2019
It translates text, answers questions, summarizes passages, and generates text output on a level which is indistinguishable from humans and can become repetitive when generating long passage
Language tasks such as reading, summarizing and translation can be learned by GPT-2 from raw text without using domain specific training data
Background: 
GPT2 is one from the series of Generative Pre-trained Model (GPT)
GPT is one of the pioneers in Language Understanding and Modeling i.e., the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence used in Natural Language Processing application ( giving computers the ability to understand text and spoken words in much the same way human beings can)
It essentially proposes the concept of pre-training ( old knowledge helps new models successfully perform new tasks from old experience instead of from scratch) a language model on a huge corpus of data and then fine-tuning ( a model that has already been trained for a particular task and then fine-tuning or tweaking it to make it perform a second similar task)

